---
output: 
  revealjs::revealjs_presentation:
    transition: none
    css: custom.css
---

# Regular guys talk regularization

```{r include = FALSE}
library(tidyverse)
library(xtable)

knitr::opts_chunk$set(
  echo = TRUE
)
```

## What we'll talk aboout

* Why regularization
* What is it?
* How do I do it?
* Tweedie
* Mathy stuff
* Conclusion

# Why?

## Why?

Let's start with some data and a model

## Data

* *dataOhlsson* from *insuranceData* R package 
* Swedish motorcycle insurance from Wasa, 1994 to 1998
* We've renamed variables to English

```{r }
library(insuranceData)
data("dataOhlsson")

# Drop the claim count variable
tbl_ohlsson <- dataOhlsson %>%
  select(
    age_number = agarald
    , territory = zon
    , motor_class = mcklass
    , vehicle_age = fordald
    , bonus_class = bonuskl
    , duration
    , losses = skadkost)
  
```

##

```{r}
str(tbl_ohlsson)
```

## Fit a model

```{r}
fit_ols <- lm(
  losses ~ .
  , data = tbl_ohlsson
)
```

## Which coefficient should we drop?

```{r}
fit_ols %>% 
  summary()
```

## Options

* Manual selection based on standard error of coefficients
* Stepwise regression
* Feature engineering
* PCA
* Partial least squares
* Or ...

## Regularization!

Benefits:

* "Curse of dimensionality" number of observations not much larger than $p$
* No p-hacking
* Let the model pick your variables!
* Reduce chance that the model will overfit
* Collinearity

# What

## What is regularization

Regluarization adjusts the cost function which creates the model

## The _what_ function?

* Models map data (predictors) to other data (target variable)
* The preferred model is one which optimizes some _cost_ of model output
* OLS cost -> least squares
* GLM -> maximum likelihood/residual deviance
* Regularization augments OLS/GLM with a penalty based on the magnitude of the coefficients

## OLS cost function

$$\sum_i^n{\left(y_i-\hat{y_i}\right)^2}=\sum_i^n{\left(y_i-\hat{\beta_0} -\sum_j^p\left(x_{ij}*\hat{\beta_j}\right)\right)}^2 = RSS $$

## Regularization cost function

$$\sum_i^n{\left(y_i-\hat{\beta_0} -\sum_j^p\left(x_{ij}*\hat{\beta_j}\right)\right)}^2 + \lambda\sum_j^p{\|\hat{\beta_j}\|}_L = RSS + \lambda\sum_j^p{\|\hat{\beta_j}\|_L}$$

## Two cost functions

**OLS**

$$\sum_i^n{\left(y_i-\hat{y_i}\right)^2}=\sum_i^n{\left(y_i-\hat{\beta_0} -\sum_j^p\left(x_{ij}*\hat{\beta_j}\right)\right)}^2 = RSS $$

**Regularization:**

$$\sum_i^n{\left(y_i-\hat{\beta_0} -\sum_j^p\left(x_{ij}*\hat{\beta_j}\right)\right)}^2 + \lambda\sum_j^p{\|\hat{\beta_j}\|}_L = RSS + \lambda\sum_j^p{\|\hat{\beta_j}\|_L}$$

## Overfitting and the role of $\lambda$

Analogue to credibility. $\lambda$ applies a shrinkage to the parameters. The "complement" is the intercept.

Same idea: reduce variance on out of sample data. 

Control weight given to predictors (i.e. $\hat{\beta_j}$), in favor of $\hat{\beta_0}$.

## L?

$$\sum_j^p{\|\hat{\beta_j}\|_L}$$

$$L=1 \implies\sum_j^p{|\hat{\beta_j}|}$$

$$L=2 \implies\sum_j^p{\hat{\beta_j}^2}$$

## L

* Can L be higher than 2?
* Must L be an integer?

## L1 and L2 norms

L1 = **L**east **A**bsolute **S**hrinkage and **S**election **O**perator = LASSO

$$L=1\implies RSS + \lambda\sum_j^p|\beta_j|$$

L2 = Ridge regression

$$L=2\implies RSS + \lambda\sum_j^p{\beta_j}^2$$

# How

## Easy answer

Use `glmnet`

```{r results = 'hide', message=FALSE}
mat_ohlsson <- tbl_ohlsson %>% 
  select(-losses) %>% 
  as.matrix()

library(glmnet)
fit_ridge <- glmnet(
  x = mat_ohlsson
  , y = tbl_ohlsson$losses
  , family = 'gaussian'
  , alpha = 0
# , lambda = seq()  
)
```

## About `alpha`

Used to mix Ridge and Lasso

$$(1 - \alpha)/2\|\beta\|^2_2 + \alpha\|\beta\|_1$$

$$\alpha = 0 \implies Ridge$$

$$\alpha = 1 \implies Lasso$$

## What does `glmnet` do?

1. Standardize the predictor space (unless you tell it not to)
2. Form a set of candidate $\lambda$'s (unless you provide your own)
3. Fit coefficients for each $\lambda$

We should:

1. Use cross validation to measure RMSE (or other metric) on out of sample (test) data
4. Pick the $\lambda$ which optimizes out of sample predictions

## Standardize predictors

* Why?
* OLS is scale-invariant, regularization isn't
* Extreme(ish) case: convert currency
* glmnet returns coefficients at the original scale.

```{r}
fit_ols %>% 
  summary()
```

## Fit using many different $\lambda$'s

```{r echo = FALSE}
library(coefplot)
ridge_copath<-fit_ridge %>%
    coefpath()
ridge_copath
```

## What does lasso look like?

```{r}
fit_lasso <- glmnet(
    x = mat_ohlsson
  , y = tbl_ohlsson$losses
  , family = 'gaussian'
  , alpha = 1
)
```

## What does lasso look like?

```{r echo = FALSE}
# fit_lasso %>% 
#   plot(xvar = 'lambda')
lasso_copath<-fit_lasso %>%
    coefpath()
lasso_copath
```


## Use cross validation to measure RMSE (or other metric) on out of sample (test) data

```{r}
fit_ridge_cv <- cv.glmnet(
    x = mat_ohlsson
  , y = tbl_ohlsson$losses
  , family = 'gaussian'
  , alpha = 0
  , nfolds = 10
# , foldid = NULL 
)

fit_lasso_cv <- cv.glmnet(
    x = mat_ohlsson
  , y = tbl_ohlsson$losses
  , family = 'gaussian'
  , alpha = 1
  , nfolds = 10
)
```

## What $\lambda$ to pick?

```{r echo=FALSE,message=FALSE}
#plot(fit_lasso_cv)
library(broom)
library(gridExtra)
plt_lasso_cv<-
  tidy(fit_lasso_cv) %>%
  rename(Mean_Sq_Error=estimate) %>%
  ggplot(aes(lambda,Mean_Sq_Error))+
  geom_line()+
  scale_x_log10()+
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .25)+
  geom_vline(xintercept=fit_lasso_cv$lambda.min)+
  geom_vline(xintercept = fit_lasso_cv$lambda.1se,linetype="dotted")+
  ggtitle("Lasso")

plt_ridge_cv<-
  tidy(fit_ridge_cv) %>%
  rename(Mean_Sq_Error=estimate) %>%
  ggplot(aes(lambda,Mean_Sq_Error))+
  geom_line()+
  scale_x_log10()+
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .25)+
  geom_vline(xintercept=fit_ridge_cv$lambda.min)+
  geom_vline(xintercept = fit_ridge_cv$lambda.1se,linetype="dotted")+
  ggtitle("Ridge")

grid.arrange(plt_ridge_cv,plt_lasso_cv,ncol=1,nrow=2)
```


## Pick $\lambda$ to optimize OoS prediction

```{r}
ridge_lambda_select<-fit_ridge_cv$lambda.min
lasso_lambda_select<-fit_lasso_cv$lambda.min
selected_coef_gauss<-
    data.frame(as.matrix(coef(fit_ridge_cv,s=ridge_lambda_select))
          ,as.matrix(coef(fit_lasso_cv,s=lasso_lambda_select)))
names(selected_coef_gauss)<-c("Ridge","Lasso")
```

```{r echo=FALSE,results='asis'}
tbl<-xtable(as_tibble(selected_coef_gauss,rownames='Variable'))
print.xtable(tbl,'html')
```

# The Tweedie distribution

## OLS but not so ordinary

```{r include=FALSE}

percent0<-(tbl_ohlsson %>% filter(losses==0)%>%nrow())/nrow(tbl_ohlsson)

```

```{r echo=FALSE}
tbl_ohlsson %>%
  filter(losses>0)%>%
  ggplot(aes(x=(losses)))+geom_density()+
  ggtitle(paste0("Distribution of non-zero losses (",scales::percent(percent0),"=0)"))+
  scale_y_continuous(labels=scales::percent)+
  scale_x_continuous(labels=scales::comma)
```

## One curve to rule them all
* Tweedie family contains any distribution that satisfies r:
$$Variance = \phi* \mu^p$$
* This includes
  - Normal: $p=0$
  - Poisson: $p=1$
  - <span style="background-color: #FFFF00">Compound Gamma/Poisson: $1<p<2$ </span>
  - Gamma: $p=2$
  - Inverse Gaussian: $p=3$

* Generally no closed form.
  
## GLM

```{r warning = FALSE}
library(statmod)

fit_glm <- glm(
  losses ~ .
  , family = tweedie(var.power = 1.5, link.power = 0)
  , data = tbl_ohlsson
)

summary(fit_glm)

```

## HDTweedie
* Package is built on glmnet, with addition of the Tweedie family
```{r}
library(HDtweedie)
fit_ridge_cv_tweedie <- cv.HDtweedie(
  x = mat_ohlsson
  , y = tbl_ohlsson$losses
  , p = 1.5
  , alpha = 0
  , lambda = seq(from=exp(0),to=exp(5),length.out = 100)
  , standardize=TRUE#
)

fit_lasso_cv_tweedie <- cv.HDtweedie(
  x = mat_ohlsson
  , y = tbl_ohlsson$losses
  , p = 1.5
  , alpha = 1
  , lambda = seq(from=exp(-2),to=exp(3),length.out = 100)
  , standardize=TRUE
)
```

## Tweedie Ridge Path
```{r include=FALSE}
{
extractPath.HDtweedie <- function(model, intercept=FALSE, ...)
{
  thePath <-   
    cbind(log(model$lambda),
    t(coef(model,
          s=model$lambda))) %>% 
    as.tibble() %>% 
    rename(lambda=V1,Intercept="(Intercept)") %>%
    select(-Intercept) %>% 
    dplyr::arrange(.data$lambda)
  
  if(!intercept)
  {
    thePath <- thePath %>% 
      dplyr::select(-dplyr::matches('(Intercept)'))
  }
  
  return(thePath)
}

coefpath.HDtweedie<-function(model,...){coefplot:::coefpath.glmnet(model)}
coefpath.cv.HDtweedie <- function(model,
                               xlab='LogLambda',
                               ylab='Coefficients',
                               showLegend=c('onmouseover', 'auto', 'always', 
                                            'follow' ,'never'),
                               annotate=TRUE,
                               colorMin='black', strokePatternMin='solid',
                               labelMin='lambda.min', locMin=c('bottom', 'top'),
                               color1se='black', strokePattern1se='dotted',
                               label1se='lambda.1se', loc1se=c('bottom', 'top'),
                               ...)
{
  # figure out how to show the legend
  showLegend <- match.arg(showLegend)
  locMin <- match.arg(locMin)
  loc1se <- match.arg(loc1se)
  
  g <- coefpath(model$HDtweedie.fit, ...)
  
  g %>% 
    dygraphs::dyEvent(x=log(model$lambda.min), label=labelMin, 
                      color=colorMin,
                      labelLoc=locMin, strokePattern=strokePatternMin) %>% 
    dygraphs::dyEvent(x=log(model$lambda.1se), label=label1se, 
                      color=color1se,
                      labelLoc=loc1se, strokePattern=strokePattern1se)
}

}
```

```{r echo = FALSE}
ridge_tweed_copath<-fit_ridge_cv_tweedie %>%
    coefpath()
ridge_tweed_copath
```

## And Lasso...
```{r echo=FALSE}
lasso_tweed_copath<-fit_lasso_cv_tweedie %>%
    coefpath()
lasso_tweed_copath

```

## The Tweedie CV Plot
```{r echo=FALSE}

plt_lasso_cv_tweed<-
tibble(lambda=fit_lasso_cv_tweedie$lambda
       ,Deviance=fit_lasso_cv_tweedie$cvm,conf.low=fit_lasso_cv_tweedie$cvlo,conf.high=fit_lasso_cv_tweedie$cvupper)%>%
  ggplot(aes(lambda,Deviance))+
  geom_line()+
  scale_x_log10()+
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .25)+
  geom_vline(xintercept=fit_lasso_cv_tweedie$lambda.min)+
  geom_vline(xintercept = fit_lasso_cv_tweedie$lambda.1se,linetype="dotted")+
  ggtitle("Lasso")
  

plt_ridge_cv_tweed <-
  tibble(lambda=fit_ridge_cv_tweedie$lambda
       ,Deviance=fit_ridge_cv_tweedie$cvm,conf.low=fit_ridge_cv_tweedie$cvlo,conf.high=fit_ridge_cv_tweedie$cvupper) %>%
  ggplot(aes(lambda,Deviance))+
  geom_line()+
  scale_x_log10()+
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .25)+
  geom_vline(xintercept=fit_ridge_cv_tweedie$lambda.min)+
  geom_vline(xintercept = fit_ridge_cv_tweedie$lambda.1se,linetype="dotted")+
  ggtitle("Ridge")

grid.arrange(plt_ridge_cv_tweed,plt_lasso_cv_tweed,ncol=1,nrow=2)

```


## What $\lambda$ to pick now?
```{r}
ridge_lambda_select<-fit_ridge_cv_tweedie$lambda.min
lasso_lambda_select<-fit_lasso_cv_tweedie$lambda.min
selected_coef_tweed<-
    data.frame(as.matrix(coef(fit_ridge_cv_tweedie,s=ridge_lambda_select))
          ,as.matrix(coef(fit_lasso_cv_tweedie,s=lasso_lambda_select)))
names(selected_coef_tweed)<-c("Ridge","Lasso")
```

```{r echo=FALSE,results='asis'}
tbl<-xtable(as_tibble(selected_coef_tweed,rownames='Variable'))
print.xtable(tbl,'html')
```

## Let's compare across families
```{r}
combind_coef<-cbind(selected_coef_gauss,selected_coef_tweed)
names(combind_coef)<-c("Ridge_Gauss","Lasso_Gauss","Ridge_Tweedie","Lasso_Tweedie")
```
```{r echo=FALSE,results='asis'}
tbl<-xtable(as_tibble(combind_coef[,c("Ridge_Gauss","Ridge_Tweedie","Lasso_Gauss","Lasso_Tweedie")],rownames='Variable'))
print.xtable(tbl,'html')
```

# Mathy stuff

## The formula again

$$\sum_i^n{\left(y_i-\hat{\beta_0} -\sum_j^p\left(x_{ij}*\hat{\beta_j}\right)\right)}^2 + \lambda\sum_j^p{\|\hat{\beta_j}\|}_L = RSS + \lambda\sum_j^p{\|\hat{\beta_j}\|_L}$$

## Equivalently

Maximize:

$$\sum_i^n{\left(y_i-\hat{\beta_0} -\sum_j^p\left(x_{ij}*\hat{\beta_j}\right)\right)}^2$$

Subject to:

$$\sum_j^p{\|\hat{\beta_j}\|_L}\leq{t}$$

## Shrink or vanish

```{r echo = FALSE}
t <- 0.6
tbl_ols <- tibble(
  beta_1 = 0.1
  , beta_2 = 0.7
)

ellipse_2 <- c(0.5, 0.5)
tbl_beta <- expand.grid(
  beta_1 = seq(-1, 1, length.out = 500)
  , beta_2 = seq(-1, 1, length.out = 500)
) %>% 
  mutate(
    lasso = abs(beta_1) + abs(beta_2) <= t
    , ridge = beta_1 ^ 2 + beta_2 ^ 2 <= t^2
  )

tbl_beta %>% 
  ggplot(aes(beta_1, beta_2), show.legend = FALSE) + 
  geom_point(data = tbl_ols, color = 'red', size = 4) +
  geom_raster(data = tbl_beta %>% filter(ridge), aes(fill = ridge), fill = 'grey', alpha = 0.5) + 
  geom_raster(data = tbl_beta %>% filter(lasso), aes(fill = lasso), fill = 'grey', alpha = 0.5) + 
  scale_y_continuous(limits = c(-0.75, 0.75)) + 
  scale_x_continuous(limits = c(-0.75, 0.75))
```

## The formulaic way of saying that

$$\beta = \frac{2x_iy_i-\lambda}{2x_i^2}$$

$$\beta = \frac{2x_iy_i}{2x_i^2+2\lambda}$$

## L0

Subject to:

$$\sum_j^p{\|\hat{\beta_j}\|_0}=\sum_j^p{I(\beta_j\neq0)}\leq{t}$$

No more than $t$ coefficients are not zero -> best subset.

## Collinearity

If we know both are important, we may not want to choose:

L1/LASSO pushes things to zero.

L2/Ridge restricts the size, but keeps both.

## Bayesian link

L1 = Bayes with Laplace prior

L2 = Bayes with normal priors

$$\prod^N_1{\Phi(y_n|\beta x_n, \sigma^2)\Phi(\beta |0, \lambda^{-1})}$$

# Conclusion

## Conclusion

* Option to consider for high-dimension data
* Choice of hyperparameter needs a fair bit of data
* `glmnet` package or `HDtweedie`

##

Thank you!

## References

* http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf
* https://web.stanford.edu/~hastie/Papers/ESLII.pdf
* https://stats.stackexchange.com/questions/163388/l2-regularization-is-equivalent-to-gaussian-prior
* https://github.com/PirateGrunt/intro_regularization